[
  {
    "objectID": "data_gathering/fdic_data_api_client.html",
    "href": "data_gathering/fdic_data_api_client.html",
    "title": "FDIC Data API Client",
    "section": "",
    "text": "By 503 Team 4\nSource: https://banks.data.fdic.gov/docs/#/"
  },
  {
    "objectID": "data_gathering/fdic_data_api_client.html#code",
    "href": "data_gathering/fdic_data_api_client.html#code",
    "title": "FDIC Data API Client",
    "section": "Code",
    "text": "Code\nImport libraries\n\nimport requests\nimport pandas as pd\nimport json\nimport urllib.parse\n\nDefine function\n\n# set endpoint\nendpoint = 'https://banks.data.fdic.gov/api/'\n\n# function\ndef get_all_financial_reporting_data(endpoint, reporting_period, fields, result_limit=5000):\n    \"\"\"\n    NC (Noninsured non-deposit commercial banks) are removed below to match the query of the FDIC site https://banks.data.fdic.gov/bankfind-suite/financialreporting\n    \"\"\"\n    \n    \n    # initialize data list\n    financials = []\n    \n    # query\n    fields_encoded = urllib.parse.quote(fields, safe='')\n    url = f'{endpoint}financials?filters=RISDATE:{reporting_period} AND !(BKCLASS:\"NC\")&fields={fields_encoded}&limit={result_limit}&sort_by=REPDTE&sort_order=ASC'\n    \n    # get response\n    response = requests.get(url)\n\n    if response.status_code == 200:\n        # response to json\n        data = response.json()\n\n        # store response data\n        financial_data = [item['data'] for item in data['data']]\n        financials.extend(financial_data)\n\n    else:\n        print(f'Request failed with status code {response.status_code}')\n        return None\n\n    return pd.DataFrame(financials)\n\nQuery the data\n\n# params\nreporting_period = '2022-12-31'\nfields = 'REPDTE,CERT,NAME,CITY,STNAME,ASSET,DEPDOM,BKCLASS,ADDRESS,ZIP,EQ,LIAB,ESTYMD,ENDEFYMD,' # no spaces between!\nresult_limit = 10000\n\n# call\nall_financial_reporting_data = get_all_financial_reporting_data(endpoint, reporting_period, fields, result_limit)\n\n# display\nif all_financial_reporting_data is not None:\n    all_financial_reporting_data = all_financial_reporting_data.sort_values('CERT')\n    display(all_financial_reporting_data.head())\n    #all_financial_reporting_data.to_csv(f'../../data/quarterly_financials/{reporting_period}_quarterly_financials.csv', index=False) # save\n\nCheck data\n\ndisplay(all_financial_reporting_data.shape)\n#column_to_check = 'ACTIVE'\n#display(pd.DataFrame({'col1': all_financial_reporting_data[column_to_check].value_counts().index, 'count': all_financial_reporting_data[column_to_check].value_counts().values}))\n\n\nGet the last quarter for the past 30 years\n\ntoday = pd.Timestamp.now().date()\n\n\n# define the start and end dates of the range (inclusive)\nstart_date = '1992-03-31'\nend_date = '2022-12-31'\n\n# generate a list of dates within the range, with a frequency of 1 quarter\ndates = pd.date_range(start=start_date, end=end_date, freq='Q')\n\n# loop\nfor date in dates:\n\n    # get end of quarter\n    end_of_quarter = date.strftime('%Y-%m-%d') # format\n    \n    fields = 'REPDTE,CERT,NAME,CITY,STNAME,ASSET,DEPDOM,BKCLASS,ADDRESS,ZIP,EQ,LIAB,ESTYMD,ENDEFYMD,EQR,' # no spaces between!\n    result_limit = 10000\n\n    # call\n    all_financial_reporting_data = get_all_financial_reporting_data(endpoint, end_of_quarter, fields, result_limit)\n\n    # display\n    if all_financial_reporting_data is not None:\n        all_financial_reporting_data = all_financial_reporting_data.sort_values('CERT')\n        all_financial_reporting_data.to_csv(f'../../data/quarterly_financials/{end_of_quarter}_quarterly_financials.csv', index=False)\n        print(f'{end_of_quarter} done')"
  },
  {
    "objectID": "plotly_plot.html",
    "href": "plotly_plot.html",
    "title": "Where is your money, and why?",
    "section": "",
    "text": "This file will work with the all_institutions.csv file created in the quarter_data_cleaning.ipynb file under code/data_cleaning/\nPlotly display by year\n\n# import\nimport pandas as pd\n\n# reading and cleaning data\ndf = pd.read_csv(\"../data/all_financials.csv\", index_col=False)\ndf = df.dropna(axis=0)\n\n# Grouping by name and state\ngrouped_df = df.groupby([\"STNAME\", \"date\"])[\"DEPDOM\", \"ASSET\"].apply(\n    lambda x: x.astype(int).sum()\n)\ngrouped_df = grouped_df.reset_index()\n\n# Adding a column for the year\ngrouped_df[\"year\"] = [pd.to_datetime(date).year for date in grouped_df.date]\n\n# To work with smaller data\n# grouped_df = grouped_df.head(100)\n\n# Display\ngrouped_df.head()\n\n/var/folders/hg/dd3yfd8j7vx8qtmvm42400j80000gn/T/ipykernel_54342/758944049.py:8: FutureWarning:\n\nIndexing with multiple keys (implicitly converted to a tuple of keys) will be deprecated, use a list instead.\n\n\n\n\n\n\n\n\n\n\nSTNAME\ndate\nDEPDOM\nASSET\nyear\n\n\n\n\n0\nALABAMA\n1992-03-31\n28888570\n34804559\n1992\n\n\n1\nALABAMA\n1992-06-30\n28782222\n35151498\n1992\n\n\n2\nALABAMA\n1992-09-30\n29034487\n35810540\n1992\n\n\n3\nALABAMA\n1992-12-31\n30059749\n36943922\n1992\n\n\n4\nALABAMA\n1993-03-31\n29888130\n37040762\n1993\n\n\n\n\n\n\n\n\n# Import\nimport plotly.graph_objects as go\n\n# initialize figure\nfig = go.Figure()\n\n\n# Empty Lists\nsliders = []\nbuttonList = []\n\n# Creating an empty visibility list of False values that we will fill\nnumYears = len(grouped_df.year.unique())\nnumStates = len(grouped_df.STNAME.unique())\nvisibility_list = [False] * numStates * numYears\n\n# Counter for instances\ncounter = 0\n\n# Add traces, one for each slider step\nfor state in grouped_df.STNAME.unique():\n    steps = []\n\n    # Filtered state dataframe\n    stateDF = grouped_df[grouped_df.STNAME == state]\n\n    # Displaying the first year for each state\n    state_first_visibility = visibility_list.copy()\n    state_first_visibility[counter] = True\n\n    # For loop by year\n    for year in stateDF.year.unique():\n        # Each year has its own visibility list\n        visibility_list1 = visibility_list.copy()\n        visibility_list1[counter] = True\n        # Adding to the counter\n        counter += 1\n\n        # Steps on the slider\n        step = {\n            \"method\": \"update\",\n            \"args\": [{\"visible\": visibility_list1}],\n            \"label\": str(year),\n        }\n        steps.append(step)\n\n        # ID\n        id = str(year) + str(state)\n\n        # Dataframe of just this year\n        dateDF = stateDF[stateDF.year == year]\n\n        # Adding the tace\n        fig.add_trace(\n            go.Scatter(\n                uid=id,\n                visible=False,\n                line=dict(color=\"green\", width=6),\n                name=\"Year: \" + str(year),\n                x=dateDF.date,\n                y=dateDF.ASSET,\n            )\n        )\n\n    # Creating a slider for the year\n    slider1 = [\n        dict(active=0, currentvalue={\"prefix\": \"Year: \"}, pad={\"t\": 50}, steps=steps)\n    ]\n    sliders.append(slider1)\n\n    # Making the button\n    tmpbutton = dict(\n        label=str(state),\n        method=\"update\",\n        args=[{\"visible\": state_first_visibility}, {\"sliders\": slider1}],\n    )\n\n    # Adding the button\n    buttonList.append(tmpbutton)\n\n# print(\"OUT OF LOOP\")\n# Make First trace visible\nfig.data[0].visible = True\n\n# DEFINE ONE SLIDE FOR EACH BUTTON\nfig.update_layout(sliders=sliders[0])\n\n# print(\"LAYOUT UPDATED\")\n# Adding all of the buttons\nfig.update_layout(\n    updatemenus=[\n        dict(\n            buttons=buttonList,\n            direction=\"down\",\n            showactive=True,\n            pad={\"r\": 10, \"t\": 10},\n            x=0.1,\n            xanchor=\"left\",\n            y=1.2,\n            yanchor=\"top\",\n        ),\n    ],\n    template=\"plotly_white\",\n    xaxis_title=\"Date\",\n    yaxis_title=\"Total Assets (USD)\",\n    title=\"Total Assets by State\",\n)\n\n\n# print(\"ABOUT TO DISPLAY\")\n\n# Displaying\nfig.show()\n\nOUT OF LOOP\nLAYOUT UPDATED\nABOUT TO DISPLAY\n\n\nUnable to display output for mime type(s): application/vnd.plotly.v1+json\n\n\nPlotly Display by Decade\n\n# import\nimport pandas as pd\n\n# Read in data\ndf = pd.read_csv(\"../data/all_financials.csv\", index_col=False)\ndf = df.dropna(axis=0)\n\n# Grouping by state and date\nstate_df = df.groupby([\"STNAME\", \"date\"])[\"DEPDOM\", \"ASSET\"].apply(\n    lambda x: x.astype(int).sum()\n)\nstate_df = state_df.reset_index()\n\n# Adding a decade column\nstate_df[\"decade\"] = [pd.to_datetime(date).year - (pd.to_datetime(date).year%10) for date in state_df.date]\n\n# For smaller data\n# grouped_df = grouped_df.head(100)\n\n# Display\nstate_df.head()\n\n/var/folders/hg/dd3yfd8j7vx8qtmvm42400j80000gn/T/ipykernel_68472/3332689799.py:9: FutureWarning:\n\nIndexing with multiple keys (implicitly converted to a tuple of keys) will be deprecated, use a list instead.\n\n\n\n\n\n\n\n\n\n\nSTNAME\ndate\nDEPDOM\nASSET\ndecade\n\n\n\n\n0\nALABAMA\n1992-03-31\n28888570\n34804559\n1990\n\n\n1\nALABAMA\n1992-06-30\n28782222\n35151498\n1990\n\n\n2\nALABAMA\n1992-09-30\n29034487\n35810540\n1990\n\n\n3\nALABAMA\n1992-12-31\n30059749\n36943922\n1990\n\n\n4\nALABAMA\n1993-03-31\n29888130\n37040762\n1990\n\n\n\n\n\n\n\n\nimport plotly.graph_objects as go\n\n# initialize figure\nfig = go.Figure()\n\n\n# Empty Lists\nsliders = []\nbuttonList = []\n\n# Creating an empty visibility list of False values that we will fill\nnumDecade = len(state_df.decade.unique())\nnumStates = len(state_df.STNAME.unique())\nvisibility_list = [False] * numStates * numDecade\n\n# Counter for instances\ncounter = 0\n\n# Add traces, one for each slider step\nfor state in state_df.STNAME.unique():\n    steps = []\n\n    # Filtered state dataframe\n    stateDF = state_df[state_df.STNAME == state]\n\n    # Displaying the first year for each state\n    state_first_visibility = visibility_list.copy()\n    state_first_visibility[counter] = True\n\n    # Decade for loop\n    for decade in stateDF.decade.unique():\n        # Each decade has its own visibility list\n        visibility_list1 = visibility_list.copy()\n        visibility_list1[counter] = True\n        # Adding to the counter\n        counter += 1\n\n        # Steps on the slider\n        step = {\n            \"method\": \"update\",\n            \"args\": [{\"visible\": visibility_list1}],\n            \"label\": str(decade),\n        }\n        steps.append(step)\n\n        # ID\n        id = str(decade) + str(state)\n\n        # Dataframe of just this decade\n        dateDF = stateDF[stateDF.decade == decade]\n\n        # Adding the trace\n        fig.add_trace(\n            go.Scatter(\n                uid=id,\n                visible=False,\n                line=dict(color=\"green\", width=6),\n                name=\"Decade: \" + str(decade),\n                x=dateDF.date,\n                y=dateDF.ASSET,\n            )\n        )\n\n    # Creating a slider for the decade\n    slider1 = [\n        dict(active=0, currentvalue={\"prefix\": \"Decade: \", \"suffix\": \"s\"}, pad={\"t\": 50}, steps=steps)\n    ]\n    sliders.append(slider1)\n\n    # Making the button\n    tmpbutton = dict(\n        label=str(state),\n        method=\"update\",\n        args=[{\"visible\": state_first_visibility}, {\"sliders\": slider1}],\n    )\n\n    # Adding the button\n    buttonList.append(tmpbutton)\n\n# print(\"OUT OF LOOP\")\n# Make First trace visible\nfig.data[0].visible = True\n\n# DEFINE ONE SLIDE FOR EACH BUTTON\nfig.update_layout(sliders=sliders[0])\n\n# print(\"LAYOUT UPDATED\")\n# Adding all of the buttons\nfig.update_layout(\n    updatemenus=[\n        dict(\n            buttons=buttonList,\n            direction=\"down\",\n            showactive=True,\n            pad={\"r\": 10, \"t\": 10},\n            x=0.1,\n            xanchor=\"left\",\n            y=1.2,\n            yanchor=\"top\",\n        ),\n    ],\n    template=\"plotly_white\",\n    xaxis_title=\"Date\",\n    yaxis_title=\"Total Assets (USD)\",\n    title=\"Total Assets by State\",\n)\n\n\n# print(\"ABOUT TO DISPLAY\")\n\n# Displaying\nfig.show()\n\nUnable to display output for mime type(s): application/vnd.plotly.v1+json\n\n\n\n# import\nimport pandas as pd\n\n# Read in data\ndf = pd.read_csv(\"../data/all_financials.csv\", index_col=False)\ndf = df.dropna(axis=0)\n\n# Grouping by state and date\ngrouped_df = df.groupby([\"STNAME\", \"NAME\", \"date\"])[\"DEPDOM\", \"ASSET\"].apply(\n    lambda x: x.astype(int).sum()\n)\ngrouped_df = grouped_df.reset_index()\n\n# Adding a decade column\ngrouped_df[\"decade\"] = [pd.to_datetime(date).year - (pd.to_datetime(date).year%10) for date in grouped_df.date]\n\n# For smaller data\n# grouped_df = grouped_df.head(100)\n\n# Display\ngrouped_df.head()\ngrouped_df.to_csv(\"../data/quarterly_banks_by_state.csv\", index=False)\n\n\nimport pandas as pd\n\nname_df = pd.read_csv(\"../data/quarterly_banks_by_state.csv\", index_col=False)\n\ntop_banks = [\"JPMORGAN CHASE BANK NA\", \"BANK OF AMERICA NA\", \"WELLS FARGO BANK NA\", \"CITIBANK NATIONAL ASSN\"]\nname_df[\"top_bank\"] = [bank if bank in top_banks else \"Other\" for bank in name_df[\"NAME\"]]\n\n\ndf2 = name_df.groupby([\"STNAME\", \"top_bank\", \"date\", \"decade\"])[\"DEPDOM\", \"ASSET\"].apply(\n    lambda x: x.astype(int).sum()\n)\ndf2 = df2.reset_index()\n\ncolor_col = []\n\nfor bank in df2[\"top_bank\"]:\n    if bank == \"JPMORGAN CHASE BANK NA\":\n        color = \"#117ACA\"\n    elif bank == \"BANK OF AMERICA NA\":\n        color = \"#E61030\"\n    elif bank == \"WELLS FARGO BANK NA\":\n        color = '#ffcc02'\n    elif bank == \"CITIBANK NATIONAL ASSN\":\n        color = \"#003A72\"\n    else:\n        color = '#118C4F'\n\n    color_col.append(color)\n\ndf2[\"color\"] = color_col\ndf2 = df2.reset_index()\ndf2.head()\n\n/var/folders/hg/dd3yfd8j7vx8qtmvm42400j80000gn/T/ipykernel_68472/2714565487.py:9: FutureWarning:\n\nIndexing with multiple keys (implicitly converted to a tuple of keys) will be deprecated, use a list instead.\n\n\n\n\n\n\n\n\n\n\nindex\nSTNAME\ntop_bank\ndate\ndecade\nDEPDOM\nASSET\ncolor\n\n\n\n\n0\n0\nALABAMA\nOther\n1992-03-31\n1990\n28888570\n34804559\n#118C4F\n\n\n1\n1\nALABAMA\nOther\n1992-06-30\n1990\n28782222\n35151498\n#118C4F\n\n\n2\n2\nALABAMA\nOther\n1992-09-30\n1990\n29034487\n35810540\n#118C4F\n\n\n3\n3\nALABAMA\nOther\n1992-12-31\n1990\n30059749\n36943922\n#118C4F\n\n\n4\n4\nALABAMA\nOther\n1993-03-31\n1990\n29888130\n37040762\n#118C4F\n\n\n\n\n\n\n\nTHREE WEIRD STATES NY AND CA\n\nimport plotly.graph_objects as go\nimport plotly.express as px\n\n# initialize figure\nfig = go.Figure()\n\n\n# Empty Lists\nsliders = []\nbuttonList = []\n\n# Creating an empty visibility list of False values that we will fill\nnumDecade = len(df2.decade.unique())\nnumStates = len(df2.STNAME.unique())\nvisibility_list = [False] * numStates * numDecade\n\n# Counter for instances\ncounter = 0\n\n# Add traces, one for each slider step\nfor state in df2.STNAME.unique():\n    steps = []\n\n    # Filtered state dataframe\n    stateDF = df2[df2.STNAME == state]\n\n    # Displaying the first year for each state\n    state_first_visibility = visibility_list.copy()\n    state_first_visibility[counter] = True\n    state_df.sort_values(by=\"date\", ascending=True, inplace=True)\n    \n    # Decade for loop\n    for decade in stateDF.decade.unique():\n        # Each decade has its own visibility list\n        visibility_list1 = visibility_list.copy()\n        visibility_list1[counter] = True\n        # Adding to the counter\n        counter += 1\n\n        # Steps on the slider\n        step = {\n            \"method\": \"update\",\n            \"args\": [{\"visible\": visibility_list1}],\n            \"label\": str(decade),\n        }\n        steps.append(step)\n\n        # ID\n        id = str(decade) + str(state)\n\n        # Dataframe of just this decade\n        dateDF = stateDF[stateDF.decade == decade]\n\n        # Adding the trace\n        fig.add_trace(\n            go.Scatter(\n                uid=id,\n                visible=False,\n                x=dateDF[\"date\"],\n                y=dateDF[\"DEPDOM\"],\n                mode=\"markers\",\n                marker_color=dateDF[\"color\"],\n                name=\"Decade: \" + str(decade)\n            )\n        )\n\n    # Creating a slider for the decade\n    slider1 = [\n        dict(\n            active=0,\n            currentvalue={\"prefix\": \"Decade: \", \"suffix\": \"s\"},\n            pad={\"t\": 50},\n            steps=steps,\n        )\n    ]\n    sliders.append(slider1)\n\n    # Making the button\n    tmpbutton = dict(\n        label=str(state),\n        method=\"update\",\n        args=[{\"visible\": state_first_visibility}, {\"sliders\": slider1}],\n    )\n\n    # Adding the button\n    buttonList.append(tmpbutton)\n\n# print(\"OUT OF LOOP\")\n# Make First trace visible\nfig.data[0].visible = True\n\n# DEFINE ONE SLIDE FOR EACH BUTTON\nfig.update_layout(sliders=sliders[0])\n\n# print(\"LAYOUT UPDATED\")\n# Adding all of the buttons\nfig.update_layout(\n    updatemenus=[\n        dict(\n            buttons=buttonList,\n            direction=\"down\",\n            showactive=True,\n            pad={\"r\": 10, \"t\": 10},\n            x=0.1,\n            xanchor=\"left\",\n            y=1.2,\n            yanchor=\"top\",\n        ),\n    ],\n    xaxis_title=\"Date\",\n    yaxis_title=\"Total Assets (USD)\",\n    title=\"Total Assets by State\",\n    template=\"plotly_white\"\n)\n\n\n# print(\"ABOUT TO DISPLAY\")\n\n# Displaying\nfig.show()\n\nUnable to display output for mime type(s): application/vnd.plotly.v1+json\n\n\n\n# Imports\nimport plotly.graph_objects as go\nimport plotly.express as px\n\n# initialize figure\nfig = go.Figure()\n\n\n# Empty Lists\nsliders = []\nbuttonList = []\n\n# Filtering to just the states we want to focus on\nstates = [\"SOUTH DAKOTA\", \"NEW YORK\", \"CALIFORNIA\", \"Texas\", \"NORTH CAROLINA\", \"OHIO\"]\n\ndf2 = df2[[True if state in states else False for state in df2[\"STNAME\"]]]\n\n# Creating an empty visibility list of False values that we will fill\nnumDecade = len(df2.decade.unique())\nnumStates = len(df2.STNAME.unique())\nvisibility_list = [False] * numStates\n\n# Counter for instances\ncounter = 0\n\n# Add traces, one for each slider step\nfor i, state in enumerate(df2.STNAME.unique()):\n    steps = []\n\n    # Filtered state dataframe\n    stateDF = df2[df2.STNAME == state]\n\n    # Displaying the first year for each state\n    state_first_visibility = visibility_list.copy()\n    state_first_visibility[counter] = True\n    state_df.sort_values(by=\"date\", ascending=True, inplace=True)\n    \n    \n    # Each  has its own visibility list\n    visibility_list1 = visibility_list.copy()\n    visibility_list1[counter] = True\n    # Adding to the counter\n    counter += 1\n\n    # Steps on the slider\n    step = {\n        \"method\": \"update\",\n        \"args\": [{\"visible\": visibility_list1}],\n        \"label\": str(decade),\n    }\n    steps.append(step)\n\n    # ID\n    id = str(state)\n\n    # Making the bank names prettier\n    name_col = []\n\n    for bank in stateDF[\"top_bank\"]:\n        if bank == \"JPMORGAN CHASE BANK NA\":\n            name = \"JP Morgan Chase\"\n        elif bank == \"BANK OF AMERICA NA\":\n            name = \"Bank of America\"\n        elif bank == \"WELLS FARGO BANK NA\":\n            name = 'Wells Fargo'\n        elif bank == \"CITIBANK NATIONAL ASSN\":\n            name = \"Citibank\"\n        else:\n            name = 'All Other Banks'\n\n        name_col.append(name)\n\n    stateDF[\"pretty_name\"] = name_col\n    stateDF = stateDF.reset_index()\n    \n    # Adding the trace\n    fig.add_trace(\n        go.Scatter(\n            uid=id,\n            visible=False,\n            x=stateDF[\"date\"],\n            y=stateDF[\"DEPDOM\"],\n            mode=\"markers\",\n            marker_color=stateDF[\"color\"],\n            showlegend=True,\n            name=stateDF[\"pretty_name\"][i]\n        )\n    )\n\n    \n\n    # Creating a slider for the decade\n    slider1 = [\n        dict(\n            active=0,\n            currentvalue={\"prefix\": \"Decade: \", \"suffix\": \"s\"},\n            pad={\"t\": 50},\n            steps=steps,\n        )\n    ]\n    sliders.append(slider1)\n\n    # Making the button\n    tmpbutton = dict(\n        label=str(state),\n        method=\"update\",\n        args=[{\"visible\": state_first_visibility}, {\"sliders\": slider1}],\n    )\n\n    # Adding the button\n    buttonList.append(tmpbutton)\n\n# print(\"OUT OF LOOP\")\n# Make First trace visible\nfig.data[0].visible = True\n\n# DEFINE ONE SLIDE FOR EACH BUTTON\nfig.update_layout(sliders=sliders[0])\n\n# print(\"LAYOUT UPDATED\")\n# Adding all of the buttons\nfig.update_layout(\n    updatemenus=[\n        dict(\n            buttons=buttonList,\n            direction=\"down\",\n            showactive=True,\n            pad={\"r\": 10, \"t\": 10},\n            x=0.1,\n            xanchor=\"left\",\n            y=1.2,\n            yanchor=\"top\",\n        ),\n    ],\n    xaxis_title=\"Date\",\n    yaxis_title=\"Total Deposits (USD)\",\n    title=\"Total Deposits by State\",\n    template=\"plotly_white\"\n)\n\n\n\n\n# print(\"ABOUT TO DISPLAY\")\n# Updating labels\nfig.update_traces(hovertemplate='Date: %{x} &lt;br&gt;Total Deposits: %{y:$.0f}&lt;extra&gt;&lt;/extra&gt;') \n                                      \n# Displaying\nfig.show()\n\n/var/folders/hg/dd3yfd8j7vx8qtmvm42400j80000gn/T/ipykernel_68472/3307451403.py:73: SettingWithCopyWarning:\n\n\nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n\n/var/folders/hg/dd3yfd8j7vx8qtmvm42400j80000gn/T/ipykernel_68472/3307451403.py:73: SettingWithCopyWarning:\n\n\nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n\n/var/folders/hg/dd3yfd8j7vx8qtmvm42400j80000gn/T/ipykernel_68472/3307451403.py:73: SettingWithCopyWarning:\n\n\nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n\n/var/folders/hg/dd3yfd8j7vx8qtmvm42400j80000gn/T/ipykernel_68472/3307451403.py:73: SettingWithCopyWarning:\n\n\nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n\n/var/folders/hg/dd3yfd8j7vx8qtmvm42400j80000gn/T/ipykernel_68472/3307451403.py:73: SettingWithCopyWarning:\n\n\nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n\n\n\nUnable to display output for mime type(s): application/vnd.plotly.v1+json"
  },
  {
    "objectID": "geospatial.html",
    "href": "geospatial.html",
    "title": "Geospatial",
    "section": "",
    "text": "Geospatial\n\nBanks have many offices. Locations mean the main office. As per the FDIC the main office is the headquarters listed in the charter. Which may not be the same as the corporate headquarter.\nAttempted to query all the addresses with geopy but estimated time would have been 16 hours of runtime.\nInstead we use the zip code coordiantes as proxy for the banks main offices.\n\nZip code data obtained from “ZIP Code Tabulation Areas” in https://www.census.gov/geographies/reference-files/time-series/geo/gazetteer-files.html\nWe use the Folium Python library - a leaflet.js wrapper - which will allow us to use OpenStreetMap.\nMany banks have moved to south dakota."
  },
  {
    "objectID": "plot-5.html",
    "href": "plot-5.html",
    "title": "Plot 5",
    "section": "",
    "text": "Plot 5\nLorem Ipsum is simply dummy text of the printing and typesetting industry. Lorem Ipsum has been the industry’s standard dummy text ever since the 1500s, when an unknown printer took a galley of type and scrambled it to make a type specimen book. It has survived not only five centuries, but also the leap into electronic typesetting, remaining essentially unchanged. It was popularised in the 1960s with the release of Letraset sheets containing Lorem Ipsum passages, and more recently with desktop publishing software like Aldus PageMaker including versions of Lorem Ipsum.\nContrary to popular belief, Lorem Ipsum is not simply random text. It has roots in a piece of classical Latin literature from 45 BC, making it over 2000 years old. Richard McClintock, a Latin professor at Hampden-Sydney College in Virginia, looked up one of the more obscure Latin words, consectetur, from a Lorem Ipsum passage, and going through the cites of the word in classical literature, discovered the undoubtable source. Lorem Ipsum comes from sections 1.10.32 and 1.10.33 of “de Finibus Bonorum et Malorum” (The Extremes of Good and Evil) by Cicero, written in 45 BC. This book is a treatise on the theory of ethics, very popular during the Renaissance. The first line of Lorem Ipsum, “Lorem ipsum dolor sit amet..”, comes from a line in section 1.10.32.\nThe standard chunk of Lorem Ipsum used since the 1500s is reproduced below for those interested. Sections 1.10.32 and 1.10.33 from “de Finibus Bonorum et Malorum” by Cicero are also reproduced in their exact original form, accompanied by English versions from the 1914 translation by H. Rackham."
  },
  {
    "objectID": "code.html",
    "href": "code.html",
    "title": "Code",
    "section": "",
    "text": "Code"
  },
  {
    "objectID": "plot-4.html",
    "href": "plot-4.html",
    "title": "Plot 4",
    "section": "",
    "text": "Plot 4\nLorem Ipsum is simply dummy text of the printing and typesetting industry. Lorem Ipsum has been the industry’s standard dummy text ever since the 1500s, when an unknown printer took a galley of type and scrambled it to make a type specimen book. It has survived not only five centuries, but also the leap into electronic typesetting, remaining essentially unchanged. It was popularised in the 1960s with the release of Letraset sheets containing Lorem Ipsum passages, and more recently with desktop publishing software like Aldus PageMaker including versions of Lorem Ipsum.\nContrary to popular belief, Lorem Ipsum is not simply random text. It has roots in a piece of classical Latin literature from 45 BC, making it over 2000 years old. Richard McClintock, a Latin professor at Hampden-Sydney College in Virginia, looked up one of the more obscure Latin words, consectetur, from a Lorem Ipsum passage, and going through the cites of the word in classical literature, discovered the undoubtable source. Lorem Ipsum comes from sections 1.10.32 and 1.10.33 of “de Finibus Bonorum et Malorum” (The Extremes of Good and Evil) by Cicero, written in 45 BC. This book is a treatise on the theory of ethics, very popular during the Renaissance. The first line of Lorem Ipsum, “Lorem ipsum dolor sit amet..”, comes from a line in section 1.10.32.\nThe standard chunk of Lorem Ipsum used since the 1500s is reproduced below for those interested. Sections 1.10.32 and 1.10.33 from “de Finibus Bonorum et Malorum” by Cicero are also reproduced in their exact original form, accompanied by English versions from the 1914 translation by H. Rackham."
  },
  {
    "objectID": "eqr.html",
    "href": "eqr.html",
    "title": "EQR Analysis",
    "section": "",
    "text": "By 503 Team 4"
  },
  {
    "objectID": "eqr.html#background",
    "href": "eqr.html#background",
    "title": "EQR Analysis",
    "section": "Background",
    "text": "Background\nA bank’s equity capital ratio, similar to a bank’s tier 1 capital ratio, is the amount of a bank’s equity as a measure of its total capital. It is a measure of a bank’s financial strength and ability to withstand economic downturns and absorb losses. The FDIC often requires a minimum 4% equity capital ratio for banks that it insures in order to allow the bank to become less reliant on debt and increase shareholder satisfaction. The equity capital ratio is an interesting figure to help project the stability of a bank."
  },
  {
    "objectID": "eqr.html#code",
    "href": "eqr.html#code",
    "title": "EQR Analysis",
    "section": "Code",
    "text": "Code\n\n#import necessary libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os\nimport altair as alt\nimport plotly.express as px\nimport plotly.graph_objects as go\nimport plotly.io as pio\npio.renderers.default = \"plotly_mimetype+notebook_connected\"\n\n# alt.data_transformers.enable('json')\nalt.data_transformers.enable('default',max_rows=None)\n\n\n#load data\n\ncsvs = []\n\nfile_path = '../data/quarterly_financials/'\nfor file in os.listdir(file_path):\n    if file.endswith('.csv'):\n        tmp_file = pd.read_csv(os.path.join(file_path, file))\n        csvs.append(tmp_file)\ndf = pd.concat(csvs, ignore_index=True)\n\n\n#preprocessing\ndf['REPDTE'] = df['REPDTE'].astype(str)\n#add year and quarter\ndf['YEAR'] = df['REPDTE'].str[:4]\ndf['QUARTER'] = ''\n\nquarters_dict = {'0331': 'Q1', '0630': 'Q2', '0930': 'Q3', '1231': 'Q4'}\ndf['QUARTER'] = df['REPDTE'].str[-4:].map(quarters_dict).fillna('ERROR')\n\ndf['YEAR'] = df['YEAR'].astype(str)\ndf['QUARTER'] = df['QUARTER'].astype(str)\ndf['QUARTER_YEAR'] = df['YEAR'] + ' ' + df['QUARTER']"
  },
  {
    "objectID": "eqr.html#top-and-bottom-20-histogram",
    "href": "eqr.html#top-and-bottom-20-histogram",
    "title": "EQR Analysis",
    "section": "Top and Bottom 20 Histogram",
    "text": "Top and Bottom 20 Histogram\n\ndf['COUNT'] = 1\ndf_eqr_overall = df.groupby('CERT').agg({'EQR' : 'mean', 'NAME': 'first', 'COUNT': 'sum', 'STNAME': 'first', 'CERT': 'first'})\ndf_eqr_overall = df_eqr_overall[df_eqr_overall['COUNT'] &gt;= 100]\n\n\nmean_value = df_eqr_overall['EQR'].mean()\nprint(df_eqr_overall['EQR'].describe())\n\nlargest = df_eqr_overall.nlargest(20, 'EQR')\nsmallest = df_eqr_overall.nsmallest(20, 'EQR')\nplot_eqr_df = pd.concat([largest, smallest], ignore_index=True)\n\nmean_row = pd.DataFrame({'EQR': [mean_value], 'NAME' : ['MEAN']})\nplot_eqr_df = pd.concat([plot_eqr_df, mean_row], ignore_index=True)\nplot_eqr_df = plot_eqr_df.sort_values(by='EQR', ascending=True)\n\n\nselection = alt.selection_single(fields=['NAME'], name='Random')\ncolor = alt.condition(selection,\n                      alt.value('#118C4F'),\n                      alt.value('#FFB90D'))\n\nchart1 = alt.Chart(plot_eqr_df).mark_bar().encode(\n    x=alt.X('NAME', sort=alt.EncodingSortField('EQR', order='ascending')),\n    y='EQR',\n    color=color,\n    tooltip=['NAME', 'STNAME:N', 'EQR']\n).properties(\n    title='20 Largest and Smallest Banks by EQR in a Quarter, with Overall Average'\n).add_selection(selection)\nchart1.encoding.x.title = 'Bank Name (MEAN denotes overall average)'\nchart1.encoding.y.title = 'Equity Capital Ratio (EQR)'\n\n\nplot_eqr_df_2 = df[df['CERT'].isin(plot_eqr_df['CERT'])]\nplot_eqr_df_2 = plot_eqr_df_2.groupby(['CERT', 'QUARTER', 'YEAR']).agg({'QUARTER_YEAR': 'first', 'EQR': 'mean', 'NAME': 'first'}).reset_index()\n\nline = (alt.Chart(plot_eqr_df_2)\n         .mark_line(strokeWidth=5, color='#118C4F')\n         .encode(x='QUARTER_YEAR:N',\n                 y=alt.Y('mean(EQR):Q', scale=alt.Scale(domain=[0,100])),\n                 color='NAME:N',\n                 tooltip=['NAME', 'QUARTER_YEAR','EQR']\n                 )\n         ).transform_filter(selection)\nline.title='Average EQR by Quarter over Past 30 Years'\nline.encoding.x.title='Year and Quarter'\nline.encoding.y.title='Equity Capital Ratio (EQR)'\nchart1 & line\n\n\nAnalysis\nFor this portion of the analysis, I chose to examine banks that represent the top and bottom 20 in average EQR by quarter. For this analysis, I filtered out any banks that do not appear in the data for more than 100 quarters (roughly 80% of the quarters). This is to eliminate any potential outliers in the data. The mean value does not all observations in the data; it only includes all banks with more than 100 quarters of data since Q1 1992. Looking at the histogram, we notice that the average EQR of 11.22 is far closer to the 20 banks that have the lowest equity capital ratio compared to the 20 banks that have the highest equity capital ratio. Looking at the 5 number summary, we see that the average (11.22) is lower than the median (10.27), indicating that there are a few banks with a far higher equity capital ratio than the majority of the rest of the banks in the FDIC database. When we more closely examine the banks that fall in the range of top or bottom 20, we see that there is no consistent state across these banks. It is noteworthy that Michigan has 2 of the 3 highest banks in terms of EQR. Another noteworthy observation is that banks in Ohio, South Dakota, and North Carolina only make up 3 of the top 20 banks in average equity capital ratio. Moreover, none of the largest banks in terms of total assets are present on this list.\n\ntop_banks = [\"JPMORGAN CHASE BANK NA\", \"BANK OF AMERICA NA\", \"WELLS FARGO BANK NA\", \"CITIBANK NATIONAL ASSN\"]\ndf_banks = df[df['NAME'].isin(top_banks)]\n\ndf_banks = df_banks.groupby(['CERT', 'QUARTER', 'YEAR']).agg({'QUARTER_YEAR': 'first', 'EQR': 'mean', 'NAME': 'first'}).reset_index()\n\nbank_colors = alt.Scale(domain=top_banks, range=[\"#117ACA\", \"#E61030\", \"#ffcc02\", \"#003A72\"])\n\nline1 = (alt.Chart(df_banks)\n         .mark_line(strokeWidth=5, color='#118C4F')\n         .encode(x='QUARTER_YEAR:N',\n                 y=alt.Y('mean(EQR):Q', scale=alt.Scale(domain=[6,35])),\n                 color=alt.Color('NAME', scale=bank_colors),\n                 tooltip=['NAME', 'QUARTER_YEAR','EQR']\n                 )\n         )\nline1.title='Average EQR by Quarter over Past 30 Years'\nline1.encoding.x.title='Year and Quarter'\nline1.encoding.y.title='Equity Capital Ratio (EQR)'"
  },
  {
    "objectID": "eqr.html#quarters-line-graph",
    "href": "eqr.html#quarters-line-graph",
    "title": "EQR Analysis",
    "section": "Quarters Line Graph",
    "text": "Quarters Line Graph\n\ndf_quarters = df.groupby(['QUARTER', 'YEAR']).agg({'QUARTER_YEAR': 'first', 'EQR': 'mean'}).reset_index()\n\nline2 = (alt.Chart(df_quarters)\n         .mark_line(strokeWidth=5, color='#118C4F')\n         .encode(x='QUARTER_YEAR:N',\n                 y=alt.Y('EQR:Q', scale=alt.Scale(domain=[6,15])),\n                 tooltip=['QUARTER_YEAR','EQR'])\n         )\nline2.title='Average EQR by Quarter over Past 30 Years'\nline2.encoding.x.title='Year and Quarter'\nline2.encoding.y.title='Equity Capital Ratio (EQR)'\nline1 & line2\n\nWe see based on the line graph above that, for the most part, the 4 major banks maintain a relatively consistent equity capital ratio. After 2004, which is the first quarter that JP Morgan Chase Bank NA is present in this data, we notice that none of the banks ever reach an equity capital ratio higher than 13, and for the most part, they have all reached a similar value between 8 and 10 since 2020. The only time any of these banks record an equity capital ratio that is well above the cumulative average is prior to the turn of the 20th century, when Bank of America NA records several quarters with an equity capital ratio over 20. Comparing the four largest banks to the overall average for all FDIC-insured banks present in the data, we see that, for the most part, the four largest banks are either identical to, or slightly below, the overall average equity capital ratio over the past 2 decades. This is due to the differing immediate needs between large national banks, such as Wells Fargo and Bank of America, and smaller, more localized banks. There are a few reasons for the diffence in focus. The first is that larger banks often have greater access to capital markets, and are thus less reliant on using their own capital to fund everyday business operations. Therefore, they do not necessarily need to maintain a high equity capital ratio. The next reason is because smaller banks often take a more conservative stance on risk management, and since equity capital ratio is a way to quantify a bank’s risk management, it can in part explain why a smaller bank’s equity capital ratio is higher, on average. Finally, the difference can be explained by differences in regulatory laws and the market/customer base that a bank is typically functioning in. Nevertheless, the FDIC does set regulations for a minimum equity capital ratio, so even the largest banks in the industry need to ensure that they have a sufficient equity capital ratio and are thus in a stronger financial position in the event of an economic downturn. While larger banks have more resources than smaller banks and thus have more methods of improving their financial position to respond to potential problems, they still must maintain sufficient equity."
  },
  {
    "objectID": "eqr.html#eqr-and-assets-small-counts",
    "href": "eqr.html#eqr-and-assets-small-counts",
    "title": "EQR Analysis",
    "section": "EQR and Assets: Small Counts",
    "text": "EQR and Assets: Small Counts\n\ndf_eqr_year = df.groupby(['CERT', 'YEAR']).agg({'EQR': 'mean', 'NAME': 'first', 'CITY': 'first', 'STNAME': 'first', 'COUNT': 'sum', 'ASSET': 'mean', 'LIAB': 'mean', 'EQ': 'mean'})\n\n\ndensity_heatmap_df = df_eqr_year[df_eqr_year['ASSET'] &lt;= 500000]\n\nfig = px.density_heatmap(\n    density_heatmap_df,\n    x=\"ASSET\",\n    y=\"EQR\",\n    marginal_x=\"histogram\",\n    marginal_y=\"histogram\",\n    color_continuous_scale=px.colors.sequential.Viridis,\n    nbinsx=50,\n    nbinsy=1000,\n    labels=dict(ASSET=\"Sum of Assets Owned\", EQR=\"Equity Capital Ratio (EQR)\"),\n    title=\"Joint distribution of Average Assets Owned and Average EQR per Year\",\n    width=650,\n    height=650,\n    range_x = [1,500000],\n    range_y = [0,15]\n)\n\nfig.show()\n\nThis plot attempts to look at smaller banks in terms of mean total assets owned over all quarters and forecast their average equity capital ratio. This heatmap contains over 200,000 unique banks, all with average total assets owned below 500,000. Based on the values of the heatmap, we see that the majority of these banks hold a small amount of assets, typically below 100,000. Nevertheless, most banks that make up this proportion maintain an equity capital ratio between 8 and 12, which is consistent relative to the average that we have previously examined."
  },
  {
    "objectID": "eqr.html#bubble-plot",
    "href": "eqr.html#bubble-plot",
    "title": "EQR Analysis",
    "section": "Bubble Plot",
    "text": "Bubble Plot\n\ndf_scatter = df.groupby(['CERT','YEAR'])\ndf_scatter = df.sort_values('NAME')\ndf_scatter = df_scatter[df_scatter['EQR']&gt;=0]\ndf_scatter = df_scatter.sort_values('YEAR')\n\nfig = px.scatter(\n    df_scatter,\n    x='EQ',\n    y='EQR',\n    animation_frame='YEAR',\n    animation_group='NAME',\n    size='EQR',\n    color='BKCLASS',\n    hover_name='NAME',\n    template='plotly_white',\n    range_color=[-100,100],\n    size_max=45\n)\nfig.update_layout(\n    title = 'Bubble Plot of Equity Capital and Equity Capital Ratio by Year',\n    xaxis_title = 'Equity Capital (EQ)',\n    yaxis_title = 'Equity Capital Ratio (EQR)',\n    legend_title_text = 'Bank Class'\n)\nfig.show()\n\nA major trend we notice when looking at this time series bubble plot is the gradual increase in quantity of banks with an equity capital ratio in the range of the average that is consistent with what we have been examining thus far. What is also noticeable is that there does not appear to be a major distinction by bank class. State chartered stock savings banks, denoted in the legend as SI, appear to always have a relatively low equity capital ratio, while commercial and federal savings banks, denoted as N, NM, and SB in the above plot, tend to have a higher equity capital ratio."
  },
  {
    "objectID": "plot-3.html",
    "href": "plot-3.html",
    "title": "Plot 3",
    "section": "",
    "text": "Plot 3\nLorem Ipsum is simply dummy text of the printing and typesetting industry. Lorem Ipsum has been the industry’s standard dummy text ever since the 1500s, when an unknown printer took a galley of type and scrambled it to make a type specimen book. It has survived not only five centuries, but also the leap into electronic typesetting, remaining essentially unchanged. It was popularised in the 1960s with the release of Letraset sheets containing Lorem Ipsum passages, and more recently with desktop publishing software like Aldus PageMaker including versions of Lorem Ipsum.\nContrary to popular belief, Lorem Ipsum is not simply random text. It has roots in a piece of classical Latin literature from 45 BC, making it over 2000 years old. Richard McClintock, a Latin professor at Hampden-Sydney College in Virginia, looked up one of the more obscure Latin words, consectetur, from a Lorem Ipsum passage, and going through the cites of the word in classical literature, discovered the undoubtable source. Lorem Ipsum comes from sections 1.10.32 and 1.10.33 of “de Finibus Bonorum et Malorum” (The Extremes of Good and Evil) by Cicero, written in 45 BC. This book is a treatise on the theory of ethics, very popular during the Renaissance. The first line of Lorem Ipsum, “Lorem ipsum dolor sit amet..”, comes from a line in section 1.10.32.\nThe standard chunk of Lorem Ipsum used since the 1500s is reproduced below for those interested. Sections 1.10.32 and 1.10.33 from “de Finibus Bonorum et Malorum” by Cicero are also reproduced in their exact original form, accompanied by English versions from the 1914 translation by H. Rackham."
  },
  {
    "objectID": "plot-2.html",
    "href": "plot-2.html",
    "title": "Plot 2",
    "section": "",
    "text": "Plot 2\nLorem Ipsum is simply dummy text of the printing and typesetting industry. Lorem Ipsum has been the industry’s standard dummy text ever since the 1500s, when an unknown printer took a galley of type and scrambled it to make a type specimen book. It has survived not only five centuries, but also the leap into electronic typesetting, remaining essentially unchanged. It was popularised in the 1960s with the release of Letraset sheets containing Lorem Ipsum passages, and more recently with desktop publishing software like Aldus PageMaker including versions of Lorem Ipsum.\nContrary to popular belief, Lorem Ipsum is not simply random text. It has roots in a piece of classical Latin literature from 45 BC, making it over 2000 years old. Richard McClintock, a Latin professor at Hampden-Sydney College in Virginia, looked up one of the more obscure Latin words, consectetur, from a Lorem Ipsum passage, and going through the cites of the word in classical literature, discovered the undoubtable source. Lorem Ipsum comes from sections 1.10.32 and 1.10.33 of “de Finibus Bonorum et Malorum” (The Extremes of Good and Evil) by Cicero, written in 45 BC. This book is a treatise on the theory of ethics, very popular during the Renaissance. The first line of Lorem Ipsum, “Lorem ipsum dolor sit amet..”, comes from a line in section 1.10.32.\nThe standard chunk of Lorem Ipsum used since the 1500s is reproduced below for those interested. Sections 1.10.32 and 1.10.33 from “de Finibus Bonorum et Malorum” by Cicero are also reproduced in their exact original form, accompanied by English versions from the 1914 translation by H. Rackham."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Where is your money, and why?",
    "section": "",
    "text": "On Friday, March 10th, 2023, U.S. regulators seized Silicon Valley Bank (SVB) in the largest U.S. bank failure since the 2008 financial crisis. The 40-year-long bank operation suddenly stopped after depositors panicked when learning that the bank was short on capital. The bank found itself in this position due to several factors, including incorrect investment allocation and customers requiring to use more cash in the bank as Venture Capital firms cut back their funding (Vanian 2023). The public quickly turned its attention to the Federal Deposit Insurance Corporation (FDIC). The FDIC is an independent government agency tasked with insuring deposits, supervising financial institutions, and dealing with the orderly management of a bank failure. Most famously, the FDIC provides a standard insurance amount of $250,000 per depositor, per insured bank, which has accomplished for no depositor to lose a penny of insured funds due to a failure since 1934 (Federal Deposit Insurance Corporation 2020). The case of the FDIC showcased just one of the fascinating aspects of the U.S. banking system that was largely unknown to the general public. It also inspired us to carry out this project. With it, we aim to inform our audience of several distinguishing facts about the U.S. banking system."
  },
  {
    "objectID": "index.html#introduction",
    "href": "index.html#introduction",
    "title": "Where is your money, and why?",
    "section": "",
    "text": "On Friday, March 10th, 2023, U.S. regulators seized Silicon Valley Bank (SVB) in the largest U.S. bank failure since the 2008 financial crisis. The 40-year-long bank operation suddenly stopped after depositors panicked when learning that the bank was short on capital. The bank found itself in this position due to several factors, including incorrect investment allocation and customers requiring to use more cash in the bank as Venture Capital firms cut back their funding (Vanian 2023). The public quickly turned its attention to the Federal Deposit Insurance Corporation (FDIC). The FDIC is an independent government agency tasked with insuring deposits, supervising financial institutions, and dealing with the orderly management of a bank failure. Most famously, the FDIC provides a standard insurance amount of $250,000 per depositor, per insured bank, which has accomplished for no depositor to lose a penny of insured funds due to a failure since 1934 (Federal Deposit Insurance Corporation 2020). The case of the FDIC showcased just one of the fascinating aspects of the U.S. banking system that was largely unknown to the general public. It also inspired us to carry out this project. With it, we aim to inform our audience of several distinguishing facts about the U.S. banking system."
  },
  {
    "objectID": "index.html#what-does-it-mean-for-money-to-be-somewhere",
    "href": "index.html#what-does-it-mean-for-money-to-be-somewhere",
    "title": "Where is your money, and why?",
    "section": "What does it mean for money to “be” somewhere?",
    "text": "What does it mean for money to “be” somewhere?\nLet us clarify that your money is actually stored as bits in bank-operated servers. Each bank needs to maintain records of their customer’s accounts, so multiple copies of this data may be stored across different servers. That is where your money actually is, but that is not very interesting. In this project, we will refer to money being where the bank is. However, that leads to a second question: What does it mean for a bank to be somewhere? Is it where the headquarters are? Is it where the executives work? Is it where most of its branches are? We adopt the Federal Reserve’s definition of a bank’s location: its main office as listed in the bank’s charter (Board of Governors of the Federal Reserve System 2022).\n\nFigure 1\n\n\n\n\n\n1992-12-31\n        \n        2002-12-31\n        \n        2012-12-31\n        \n        2022-12-31\n        \n        \n\n\n\n\nFigure 2"
  },
  {
    "objectID": "index.html#a-story-about-the-top-5-banks",
    "href": "index.html#a-story-about-the-top-5-banks",
    "title": "Where is your money, and why?",
    "section": "A story about the top 5 banks",
    "text": "A story about the top 5 banks"
  },
  {
    "objectID": "index.html#what-about-deposits",
    "href": "index.html#what-about-deposits",
    "title": "Where is your money, and why?",
    "section": "What about deposits?",
    "text": "What about deposits?"
  },
  {
    "objectID": "index.html#should-we-be-worried",
    "href": "index.html#should-we-be-worried",
    "title": "Where is your money, and why?",
    "section": "Should we be worried?",
    "text": "Should we be worried?"
  },
  {
    "objectID": "methods.html",
    "href": "methods.html",
    "title": "Methods",
    "section": "",
    "text": "Methods"
  },
  {
    "objectID": "authors.html",
    "href": "authors.html",
    "title": "Authors",
    "section": "",
    "text": "Authors\n\n\n\nAustin Barish\n I am currently a senior at Georgetown majoring in Economics and Philosophy while pursuing Georgetown’s accelerated Data Science degree. I am originally from Denver, Colorado, and love sports, particularly skiing. I have been in DC for the past 3-ish years now and absolutely love it. I am particularly interested in economic research, sports data, and environmental causes. abb110@georgetown.edu\n\n\nJordan Rinaldi\n I am in the 5-year combined/accelerated program at DSAN, meaning that I am currently an undergraduate senior at Georgetown who is also taking graduate classes. I am majoring in math and double minoring in economics and government. I hope to enter the field of data science and analytics, preferably on the more data side of the industry. jar388@georgetown.edu\n\n\n\n\n\n\nRaunak Advani\n I’m currently a senior at Georgetown University completing my undergraduate degree in Economics with a minor in Psychology. I’m currently enrolled in Georgetown’s Accelerated BA/MS DSAN program, which means that I will complete both my BA in Economics and MS in DSAN in 5 years. Originally from Mumbai, India, I first come to the United States to attend boarding school in Connecticut after finishing my 10th grade back home. ra1113@georgetown.edu\n\n\nVictor De Lima\n I am a currently a first year student at the MS in Data Science and Analytics program. I am very interested in how machine learning models work. I want to play a part in how these models can be made better and smarter. I am hoping to lay a strong foundation for this during my time at the DSAN program. Some of my other interests are science in general, physics, technology, traveling, and history. I also love programming. vad49@georgetown.edu"
  },
  {
    "objectID": "data_cleaning/quarterly_data_cleaning.html",
    "href": "data_cleaning/quarterly_data_cleaning.html",
    "title": "Where is your money, and why?",
    "section": "",
    "text": "This file will compile all of the quarterly csv files into one giant csv for analysis.\n\n# Imports\nimport pandas as pd\nimport numpy as np\nimport os\n\n# Setting the path and getting csv names\nfinancials_path = \"../../data/quarterly_financials/\"\nquarterly_data_files = os.listdir(financials_path)\n\n# Stripping the file names for their dates\ndates = []\nfor date in quarterly_data_files:\n    try:\n        new_date = pd.to_datetime(date.strip(\"_quarterly_financials.csv\"))\n\n    except ValueError:\n        new_date = pd.to_datetime(\"2012-09-30\")\n\n    dates.append(new_date)\n# dates = pd.to_datetime([date.strip(\"_quarterly_financials.csv\") for date in quarterly_data_files])\n\n# For Loop to go through files\nfor i, file in enumerate(quarterly_data_files):\n    # individual file df\n    file_df = pd.read_csv(financials_path + file)\n    \n    # Date for the file\n    date = dates[i]\n\n    # Adding columns to denote the quarter and date of the institution/row\n    file_df[\"date\"] = np.repeat(date, len(file_df))\n    \n    file_df[\"quarter\"] = np.repeat(date.quarter, len(file_df))\n\n    # Combining all the data\n    if file == quarterly_data_files[0]:\n        df = file_df\n    else:\n        df = pd.concat([df, file_df], ignore_index=True)\n\n# Saving the file\ndf.to_csv(\"../../data/all_financials.csv\", index=False)"
  },
  {
    "objectID": "data_cleaning/geospatial_cleaning.html",
    "href": "data_cleaning/geospatial_cleaning.html",
    "title": "Data Cleaning - Geospatial Data",
    "section": "",
    "text": "import pandas as pd\nimport os\nimport geopandas as gpd\nimport folium\nfrom folium.plugins import HeatMap\nfrom IPython.display import display, HTML, IFrame\nfrom io import BytesIO\nimport base64\n# specify the directory containing the CSV files\ndirectory = '../../data/quarterly_financials'\n\n# create an empty list to store the dataframes\ndfs = []\n\n# loop over the CSV files in the directory\nfor filename in os.listdir(directory):\n    if filename.endswith('.csv'):\n        # read the CSV file into a dataframe and append it to the list\n        path = os.path.join(directory, filename)\n        df = pd.read_csv(path)\n        dfs.append(df)\n\n# concatenate the dataframes into a single dataframe\nquarterly_financials = pd.concat(dfs, ignore_index=True)\n# sort\nquarterly_financials = quarterly_financials.sort_values(\n    by=['REPDTE', 'CERT'], ascending=[False, True])\nquarterly_financials = quarterly_financials.reset_index(drop=True)\n# print the combined dataframe\ndisplay(quarterly_financials.shape)\ndisplay(quarterly_financials.head())\n\n(1014549, 16)\n\n\n\n\n\n\n\n\n\nZIP\nBKCLASS\nEQR\nREPDTE\nDEPDOM\nASSET\nSTNAME\nEQ\nNAME\nCITY\nADDRESS\nENDEFYMD\nCERT\nESTYMD\nLIAB\nID\n\n\n\n\n0\n2111\nSM\n8.918529\n20221231\n163284000.0\n298020000\nMASSACHUSETTS\n26579000.0\nSTATE STREET BANK&TRUST CO\nBOSTON\n1 LINCOLN ST\n99991231.0\n14\n17920101\n271441000.0\n14_20221231\n\n\n1\n36830\nSM\n6.446081\n20221231\n952037.0\n1023366\nALABAMA\n65967.0\nAUBURNBANK\nAUBURN\n100 N GAY ST\n99991231.0\n35\n19070103\n957399.0\n35_20221231\n\n\n2\n36732\nNM\n7.323604\n20221231\n407949.0\n444822\nALABAMA\n32577.0\nROBERTSON BANKING CO\nDEMOPOLIS\n216 N WALNUT AVE\n99991231.0\n39\n18700101\n412245.0\n39_20221231\n\n\n3\n36867\nNM\n-3.191064\n20221231\n266874.0\n265272\nALABAMA\n-8465.0\nPHENIX-GIRARD BANK\nPHENIX CITY\n801 13TH ST\n99991231.0\n41\n19040504\n273737.0\n41_20221231\n\n\n4\n36401\nNM\n7.197104\n20221231\n70649.0\n76239\nALABAMA\n5487.0\nBANK OF EVERGREEN\nEVERGREEN\n146 W FRONT ST\n99991231.0\n49\n19320901\n70752.0\n49_20221231\ndisplay(len(quarterly_financials['BKCLASS'].unique()))\ndisplay(quarterly_financials['BKCLASS'].unique())\n\n7\n\n\narray(['SM', 'NM', 'N', 'SI', 'SB', 'SL', 'OI'], dtype=object)\nZip coordinates\n# bring in\nzip_coordinates = pd.read_csv(\n    '../../data/coordinate_data/2022_Gaz_zcta_national.txt', sep='\\t')\n\n# remove whitespace in col names\nzip_coordinates.columns = [col.strip() for col in zip_coordinates.columns]\n\n# drop unnecesary columns\nzip_coordinates.drop(['ALAND', 'ALAND_SQMI', 'AWATER',\n                     'AWATER_SQMI'], axis=1, inplace=True)\n\n# check\ndisplay(zip_coordinates.shape)\ndisplay(zip_coordinates.head())\n\n(33791, 3)\n\n\n\n\n\n\n\n\n\nGEOID\nINTPTLAT\nINTPTLONG\n\n\n\n\n0\n601\n18.180555\n-66.749961\n\n\n1\n602\n18.361945\n-67.175597\n\n\n2\n603\n18.457399\n-67.124867\n\n\n3\n606\n18.158327\n-66.932928\n\n\n4\n610\n18.293960\n-67.127182\ndisplay(len(quarterly_financials['ZIP'].unique()))\n\n10756\nCombine\n# Rename the 'GEOID' column in 'zip_coordinates' to match the 'ZIP' column in 'quarterly_financials'\nzip_coordinates = zip_coordinates.rename(columns={'GEOID': 'ZIP'})\n\n# Merge the two dataframes on the 'ZIP' column\nmerged_df = quarterly_financials.merge(zip_coordinates, on='ZIP', how='left')\n\n# Create a new column 'coordinates' with the combined 'INTPTLAT' and 'INTPTLONG' columns as a tuple\nmerged_df['zip_coordinates'] = list(\n    zip(merged_df['INTPTLAT'], merged_df['INTPTLONG']))\n\n# Update the 'quarterly_financials' dataframe with the new 'coordinates' column\nquarterly_financials = merged_df\n\n# Print the updated 'quarterly_financials' dataframe\ndisplay(quarterly_financials.shape)\ndisplay(quarterly_financials.head())\n\n(1014549, 19)\n\n\n\n\n\n\n\n\n\nZIP\nBKCLASS\nEQR\nREPDTE\nDEPDOM\nASSET\nSTNAME\nEQ\nNAME\nCITY\nADDRESS\nENDEFYMD\nCERT\nESTYMD\nLIAB\nID\nINTPTLAT\nINTPTLONG\nzip_coordinates\n\n\n\n\n0\n2111\nSM\n8.918529\n20221231\n163284000.0\n298020000\nMASSACHUSETTS\n26579000.0\nSTATE STREET BANK&TRUST CO\nBOSTON\n1 LINCOLN ST\n99991231.0\n14\n17920101\n271441000.0\n14_20221231\n42.350680\n-71.060527\n(42.35068, -71.060527)\n\n\n1\n36830\nSM\n6.446081\n20221231\n952037.0\n1023366\nALABAMA\n65967.0\nAUBURNBANK\nAUBURN\n100 N GAY ST\n99991231.0\n35\n19070103\n957399.0\n35_20221231\n32.534872\n-85.493755\n(32.534872, -85.493755)\n\n\n2\n36732\nNM\n7.323604\n20221231\n407949.0\n444822\nALABAMA\n32577.0\nROBERTSON BANKING CO\nDEMOPOLIS\n216 N WALNUT AVE\n99991231.0\n39\n18700101\n412245.0\n39_20221231\n32.417456\n-87.892213\n(32.417456, -87.892213)\n\n\n3\n36867\nNM\n-3.191064\n20221231\n266874.0\n265272\nALABAMA\n-8465.0\nPHENIX-GIRARD BANK\nPHENIX CITY\n801 13TH ST\n99991231.0\n41\n19040504\n273737.0\n41_20221231\n32.498054\n-85.023590\n(32.498054, -85.02359)\n\n\n4\n36401\nNM\n7.197104\n20221231\n70649.0\n76239\nALABAMA\n5487.0\nBANK OF EVERGREEN\nEVERGREEN\n146 W FRONT ST\n99991231.0\n49\n19320901\n70752.0\n49_20221231\n31.468970\n-86.950426\n(31.46897, -86.950426)\n# removed unmatched zips\nquarterly_financials.dropna(subset=['INTPTLAT'], inplace=True)\n\n# drop unnecesary columns\nquarterly_financials.drop(['INTPTLAT', 'INTPTLONG', 'ENDEFYMD', 'CERT',\n                          'ESTYMD', 'LIAB', 'ID', 'ADDRESS', 'EQ', 'ZIP'], axis=1, inplace=True)\n\n# convert numerical values to millions (source is in thousands)\nquarterly_financials['DEPDOM'] = quarterly_financials['DEPDOM'].divide(1000)\nquarterly_financials['ASSET'] = quarterly_financials['ASSET'].divide(1000)\n\n# rename\nquarterly_financials.rename(columns={\n    'BKCLASS': 'bank_class',\n    'REPDTE': 'report_date',\n    'DEPDOM': 'deposits_mill',\n    'ASSET': 'assets_mill',\n    'STNAME': 'state',\n    'NAME': 'name',\n    'CITY': 'city'\n}, inplace=True)\n\ndisplay(quarterly_financials.head())\n\n\n\n\n\n\n\n\nbank_class\nEQR\nreport_date\ndeposits_mill\nassets_mill\nstate\nname\ncity\nzip_coordinates\n\n\n\n\n0\nSM\n8.918529\n20221231\n163284.000\n298020.000\nMASSACHUSETTS\nSTATE STREET BANK&TRUST CO\nBOSTON\n(42.35068, -71.060527)\n\n\n1\nSM\n6.446081\n20221231\n952.037\n1023.366\nALABAMA\nAUBURNBANK\nAUBURN\n(32.534872, -85.493755)\n\n\n2\nNM\n7.323604\n20221231\n407.949\n444.822\nALABAMA\nROBERTSON BANKING CO\nDEMOPOLIS\n(32.417456, -87.892213)\n\n\n3\nNM\n-3.191064\n20221231\n266.874\n265.272\nALABAMA\nPHENIX-GIRARD BANK\nPHENIX CITY\n(32.498054, -85.02359)\n\n\n4\nNM\n7.197104\n20221231\n70.649\n76.239\nALABAMA\nBANK OF EVERGREEN\nEVERGREEN\n(31.46897, -86.950426)\n# change to title case\nquarterly_financials[['state', 'name', 'city']] = quarterly_financials[[\n    'state', 'name', 'city']].apply(lambda x: x.str.title())\n\n# format date\nquarterly_financials['report_date'] = pd.to_datetime(\n    quarterly_financials['report_date'], format='%Y%m%d')\n\n# set to float\nquarterly_financials['deposits_mill'] = quarterly_financials['deposits_mill'].astype(\n    float)\nquarterly_financials['assets_mill'] = quarterly_financials['assets_mill'].astype(\n    float)\n\n\ndisplay(quarterly_financials.head())\n\n\n\n\n\n\n\n\nbank_class\nEQR\nreport_date\ndeposits_mill\nassets_mill\nstate\nname\ncity\nzip_coordinates\n\n\n\n\n0\nSM\n8.918529\n2022-12-31\n163284.000\n298020.000\nMassachusetts\nState Street Bank&Trust Co\nBoston\n(42.35068, -71.060527)\n\n\n1\nSM\n6.446081\n2022-12-31\n952.037\n1023.366\nAlabama\nAuburnbank\nAuburn\n(32.534872, -85.493755)\n\n\n2\nNM\n7.323604\n2022-12-31\n407.949\n444.822\nAlabama\nRobertson Banking Co\nDemopolis\n(32.417456, -87.892213)\n\n\n3\nNM\n-3.191064\n2022-12-31\n266.874\n265.272\nAlabama\nPhenix-Girard Bank\nPhenix City\n(32.498054, -85.02359)\n\n\n4\nNM\n7.197104\n2022-12-31\n70.649\n76.239\nAlabama\nBank Of Evergreen\nEvergreen\n(31.46897, -86.950426)\nquarterly_financials['bank_class'] = quarterly_financials['bank_class'].replace({\n    'N':  'Commercial bank, national charter, Fed member',\n    'NM': 'Commercial bank, state charter, Fed non-member',\n    'OI': 'Insured U.S. branch of a foreign chartered institution',\n    'SB': 'Federal savings banks',\n    'SI': 'State chartered stock savings banks',\n    'SL': 'State chartered stock savings and loan association',\n    'SM': 'Commercial bank, state charter, Fed member',\n    'NC': 'Noninsured non-deposit commercial bank',\n    'NS': 'Noninsured stock savings bank',\n    'CU': 'State or federally chartered credit union',\n})\n\ndisplay(quarterly_financials.shape)\ndisplay(quarterly_financials.head())\n\n(979599, 9)\n\n\n\n\n\n\n\n\n\nbank_class\nEQR\nreport_date\ndeposits_mill\nassets_mill\nstate\nname\ncity\nzip_coordinates\n\n\n\n\n0\nCommercial bank, state charter, Fed member\n8.918529\n2022-12-31\n163284.000\n298020.000\nMassachusetts\nState Street Bank&Trust Co\nBoston\n(42.35068, -71.060527)\n\n\n1\nCommercial bank, state charter, Fed member\n6.446081\n2022-12-31\n952.037\n1023.366\nAlabama\nAuburnbank\nAuburn\n(32.534872, -85.493755)\n\n\n2\nCommercial bank, state charter, Fed non-member\n7.323604\n2022-12-31\n407.949\n444.822\nAlabama\nRobertson Banking Co\nDemopolis\n(32.417456, -87.892213)\n\n\n3\nCommercial bank, state charter, Fed non-member\n-3.191064\n2022-12-31\n266.874\n265.272\nAlabama\nPhenix-Girard Bank\nPhenix City\n(32.498054, -85.02359)\n\n\n4\nCommercial bank, state charter, Fed non-member\n7.197104\n2022-12-31\n70.649\n76.239\nAlabama\nBank Of Evergreen\nEvergreen\n(31.46897, -86.950426)\nquarterly_financials['zip_coordinates'].isna().sum()\n\n0"
  },
  {
    "objectID": "data_cleaning/geospatial_cleaning.html#growth-of-assets-per-state",
    "href": "data_cleaning/geospatial_cleaning.html#growth-of-assets-per-state",
    "title": "Data Cleaning - Geospatial Data",
    "section": "Growth of Assets Per State",
    "text": "Growth of Assets Per State\n\n# convert financials to geodataframe\ngdf = gpd.GeoDataFrame(quarterly_financials, geometry=gpd.points_from_xy(quarterly_financials.zip_coordinates.apply(\n    lambda p: p[1]), quarterly_financials.zip_coordinates.apply(lambda p: p[0])))\n\n# define a function to get the aggregate assets\n\n\ndef plot_bank_assets_by_date(date):\n\n    # filter geodataframe and group by state\n    filtered_gdf = gdf[gdf['report_date'] == date]\n    state_assets = filtered_gdf.groupby(\n        'state')['assets_mill'].sum().reset_index()\n\n    # create base folium map\n    m = folium.Map(location=[37.8, -96], zoom_start=3)\n\n    # function to scale the bubbles\n    def scale_bubble_size(assets):\n        return assets / 50000\n\n    # plot the bubbles\n    for index, row in state_assets.iterrows():\n        state_data = filtered_gdf[filtered_gdf['state'] == row['state']]\n        state_centroid = state_data.unary_union.centroid\n        folium.CircleMarker(\n            location=[state_centroid.y, state_centroid.x],\n            radius=scale_bubble_size(row['assets_mill']),\n            color='#118C4F',\n            fill=True,\n            fill_color='#118C4F',\n            fill_opacity=0.5,\n            popup=f\"State: {row['state']}&lt;br&gt;Total Assets: {row['assets_mill']:.0f}&lt;br&gt;Date: {date}\"\n        ).add_to(m)\n\n    # show\n    return m\n\n\n# define dates to show in the grid\nunique_dates = ['1992-12-31', '2002-12-31', '2012-12-31', '2022-12-31']\n\n# init maps\nmaps = []\nfor date in unique_dates:\n    maps.append(plot_bank_assets_by_date(pd.to_datetime(date)))\n\n\n# for rendering in HTML\ndef folium_map_to_iframe_data(m, map_name):\n    map_path = f'geospatial_data_html/{map_name}.html'\n    m.save(map_path)\n    iframe = IFrame(src=map_path, width=350, height=300)\n    return iframe\n\n\n# init html structure\nhtml_structure = \"\"\"\n&lt;style&gt;\n    .map-container {\n        display: flex;\n        flex-wrap: wrap;\n    }\n    .map-box {\n        width: 50%;\n        padding: 5px;\n        box-sizing: border-box;\n    }\n    .map-title {\n        text-align: center;\n        font-weight: bold;\n    }\n&lt;/style&gt;\n&lt;div class=\"map-container\"&gt;\n\"\"\"\n\n# add iframes to the html\nfor i, m in enumerate(maps):\n    map_name = f'map_{i}'\n    iframe_data = folium_map_to_iframe_data(m, map_name)\n    date = unique_dates[i]\n    html_structure += f'&lt;div class=\"map-box\"&gt;&lt;div class=\"map-title\"&gt;{date}&lt;/div&gt;{iframe_data._repr_html_()}&lt;/div&gt;'\n\n# close html\nhtml_structure += '&lt;/div&gt;'\n# show\ndisplay(HTML(html_structure))\n\n\n# alternative for checking in jupyter notebook\nif True == False:  # only if required\n\n    def folium_map_to_iframe_data(m):\n        data = BytesIO()\n        m.save(data, close_file=False)\n        data.seek(0)\n        b64 = base64.b64encode(data.read()).decode(\"utf-8\")\n        return f'&lt;iframe src=\"data:text/html;base64,{b64}\" width=\"350\" height=\"300\"&gt;&lt;/iframe&gt;'\n\n    table_html = '&lt;table&gt;&lt;tr&gt;'\n    for i, m in enumerate(maps):\n        iframe_data = folium_map_to_iframe_data(m)\n        table_html += f'&lt;td&gt;&lt;h3&gt;{unique_dates[i]}&lt;/h3&gt;{iframe_data}&lt;/td&gt;'\n        if (i + 1) % 2 == 0:\n            table_html += '&lt;/tr&gt;&lt;tr&gt;'\n    table_html += '&lt;/tr&gt;&lt;/table&gt;'\n\n    display(HTML(table_html))\n\n\n\n\n1992-12-31\n        \n        2002-12-31\n        \n        2012-12-31\n        \n        2022-12-31"
  },
  {
    "objectID": "data_cleaning/geospatial_cleaning.html#individual-points",
    "href": "data_cleaning/geospatial_cleaning.html#individual-points",
    "title": "Data Cleaning - Geospatial Data",
    "section": "Individual Points",
    "text": "Individual Points\nHeatmap all dates\nLatest date only\n\n# get the latest date\nlatest_date = quarterly_financials['report_date'].max()\n\n# filter dataframe\nlatest_quarterly_financials = quarterly_financials[quarterly_financials['report_date'] == latest_date]\n\n# create base folium map\nmap = folium.Map(location=[37.8, -96], zoom_start=4)\n\n# heatmap data setup\nheatmap_data = []\nfor index, row in latest_quarterly_financials.iterrows():\n    coordinates = row['zip_coordinates']\n    assets_mill = row['assets_mill']\n    heatmap_data.append((*coordinates, assets_mill))\n\n# add heatmap to folium\nheatmap = HeatMap(heatmap_data, radius=13, max_zoom=13, gradient={\n                  0.2: 'blue', 0.4: 'lime', 0.6: 'orange', 1: 'red'})\nmap.add_child(heatmap)\n\n# scale the bubbles\ndef scale_bubble_size(assets):\n    return assets / 50000\n\n# add the circles with tootltips\nfor index, row in latest_quarterly_financials.iterrows():\n    coordinates = row['zip_coordinates']\n    assets_mill = row['assets_mill']\n    bank_name = row['name']\n    tooltip_text = f\"Bank: {bank_name}&lt;br&gt;Coordinates: {coordinates}&lt;br&gt;Assets (millions): {assets_mill}\"\n    folium.CircleMarker(\n        location=coordinates,\n        radius=scale_bubble_size(assets_mill),\n        color='#118C4F',\n        fill=True,\n        fill_color='#118C4F',\n        fill_opacity=0.5,\n        tooltip=tooltip_text,\n    ).add_to(map)\n\n# for rendering on Quarto\nmap.save('geospatial_data_html/heatmap.html')\nIFrame(src='geospatial_data_html/heatmap.html', width=700, height=600)\n\n# for rendering in Jupyter Notebook\ndisplay(map)\n\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook"
  },
  {
    "objectID": "data.html",
    "href": "data.html",
    "title": "Data",
    "section": "",
    "text": "We gathered the data from the FDIC’s API (Federal Deposit Insurance Corporation 2023). The API allows querying financial institutions’ general and structural data, historical financial information, and bank failure information. In this study, we focus on historical financial information. The FDIC API offers a comprehensive menu of fields covering all the previously mentioned topics, from financial ratios, income and balance sheet data, bank structure information, and more (field list available here).\nEven though the website offers a query and CSV-download user interface, we constructed our own Python API client to make the process faster (available here). The client uses the Python requests library to bring in the API’s json response, where function get_all_financial_reporting_data() processes the endpoint, reporting period, fields, and result_limit as required.\nWe then collected the quarterly financial information for all insured banks from Q1 1992 to Q4 2022 (30 years). The data is available here. Next, we cleaned the data to combine the 30 years’ worth of data, which totaled 1,014,549 rows. We collected 16 data fields (columns) described as follows:\n\n\n\n\n\n\n\n\nField name\nCode\nDescription\n\n\n\n\nZip code\nZIP\nThe first three, four, or five digits of the full postal zip code representing physical location of the institution or one of its branch offices.\n\n\nInstitution class\nBKCLASS\nA classification code assigned by the FDIC based on the institution’s charter type.\n\n\nEquity capital ratio\nEQR\nPercentage calculated as the ratio of Equity capital and Total assets.\n\n\nReport Date\nREPDTE\nThe last day of the financial reporting period selected.\n\n\nDeposits held in domestic offices\nDEPDOM\nThe sum of all domestic office deposits, including demand deposits, money market deposits, other savings deposits and time deposits.\n\n\nTotal assets\nASSET\nThe sum of all assets owned by the institution including cash, loans, securities, bank premises and other assets. This total does not include off-balance-sheet accounts.\n\n\nState name\nSTNAME\nState in which the institution or branch is physically located.\n\n\nEquity capital\nEQ\nTotal equity capital (includes preferred and common stock, surplus and undivided profits).\n\n\nInstitution name\nNAME\nThe legal title or name of the institution.\n\n\nCity\nCITY\nThe city in which an institution or branch office is physically located.\n\n\nAddress\nADDRESS\nThe street address in which an institution or branch office is physically located.\n\n\nInactive date\nENDEFYMD\nThe date that ends or closes out the last structural event relating to an institution. For closed institutions, this date represents the day that the institution became inactive.\n\n\nFDIC Certificate #\nCERT\nA unique NUMBER assigned by the FDIC used to identify institutions and for the issuance of insurance certificates.\n\n\nEstablished date\nESTYMD\nThe date on which the institution or branch began operations.\n\n\nTotal liabilities\nLIAB\nThe sum of all liabilities owed by the institution\n\n\nRecord ID\nID\nID identifying the institution and date of filing.\n\n\n\n\n\n\nWe collected coordinate data for developing geospatial visualizations. The data consists of zip codes and cartographic boundaries. The zip code data originates from the ZIP Code Tabulation Areas based on the 2020 Census tabulation blocks, available in US Census Bureau (2022). The cartographic boundaries consist of a shapefile describing states at the 500k resolution level, available in US Census Bureau (2021)."
  },
  {
    "objectID": "data.html#banking-data",
    "href": "data.html#banking-data",
    "title": "Data",
    "section": "",
    "text": "We gathered the data from the FDIC’s API (Federal Deposit Insurance Corporation 2023). The API allows querying financial institutions’ general and structural data, historical financial information, and bank failure information. In this study, we focus on historical financial information. The FDIC API offers a comprehensive menu of fields covering all the previously mentioned topics, from financial ratios, income and balance sheet data, bank structure information, and more (field list available here).\nEven though the website offers a query and CSV-download user interface, we constructed our own Python API client to make the process faster (available here). The client uses the Python requests library to bring in the API’s json response, where function get_all_financial_reporting_data() processes the endpoint, reporting period, fields, and result_limit as required.\nWe then collected the quarterly financial information for all insured banks from Q1 1992 to Q4 2022 (30 years). The data is available here. Next, we cleaned the data to combine the 30 years’ worth of data, which totaled 1,014,549 rows. We collected 16 data fields (columns) described as follows:\n\n\n\n\n\n\n\n\nField name\nCode\nDescription\n\n\n\n\nZip code\nZIP\nThe first three, four, or five digits of the full postal zip code representing physical location of the institution or one of its branch offices.\n\n\nInstitution class\nBKCLASS\nA classification code assigned by the FDIC based on the institution’s charter type.\n\n\nEquity capital ratio\nEQR\nPercentage calculated as the ratio of Equity capital and Total assets.\n\n\nReport Date\nREPDTE\nThe last day of the financial reporting period selected.\n\n\nDeposits held in domestic offices\nDEPDOM\nThe sum of all domestic office deposits, including demand deposits, money market deposits, other savings deposits and time deposits.\n\n\nTotal assets\nASSET\nThe sum of all assets owned by the institution including cash, loans, securities, bank premises and other assets. This total does not include off-balance-sheet accounts.\n\n\nState name\nSTNAME\nState in which the institution or branch is physically located.\n\n\nEquity capital\nEQ\nTotal equity capital (includes preferred and common stock, surplus and undivided profits).\n\n\nInstitution name\nNAME\nThe legal title or name of the institution.\n\n\nCity\nCITY\nThe city in which an institution or branch office is physically located.\n\n\nAddress\nADDRESS\nThe street address in which an institution or branch office is physically located.\n\n\nInactive date\nENDEFYMD\nThe date that ends or closes out the last structural event relating to an institution. For closed institutions, this date represents the day that the institution became inactive.\n\n\nFDIC Certificate #\nCERT\nA unique NUMBER assigned by the FDIC used to identify institutions and for the issuance of insurance certificates.\n\n\nEstablished date\nESTYMD\nThe date on which the institution or branch began operations.\n\n\nTotal liabilities\nLIAB\nThe sum of all liabilities owed by the institution\n\n\nRecord ID\nID\nID identifying the institution and date of filing."
  },
  {
    "objectID": "data.html#zip-coordinates",
    "href": "data.html#zip-coordinates",
    "title": "Data",
    "section": "",
    "text": "We collected coordinate data for developing geospatial visualizations. The data consists of zip codes and cartographic boundaries. The zip code data originates from the ZIP Code Tabulation Areas based on the 2020 Census tabulation blocks, available in US Census Bureau (2022). The cartographic boundaries consist of a shapefile describing states at the 500k resolution level, available in US Census Bureau (2021)."
  }
]